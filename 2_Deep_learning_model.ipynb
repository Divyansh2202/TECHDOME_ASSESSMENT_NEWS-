{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I created a small data for this assessment , data is small and imbalanced But\n",
    "# Approach and method are best i use everything for model enhancement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Input_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Title</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most economical spells at T20 World Cup - Whic...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Four overs, three wickets, no run: Lockie Ferg...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Achievement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economical Alternative to Traditional Cleanroo...</td>\n",
       "      <td>Economy</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T20 WC: Most economical four-over spells by In...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This 2002 Cirrus SR22 G1 Is a Surprisingly Eco...</td>\n",
       "      <td>Economical</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          News Title    Category Sentiment  \\\n",
       "0  Most economical spells at T20 World Cup - Whic...      Sports   Neutral   \n",
       "1  Four overs, three wickets, no run: Lockie Ferg...      Sports  Positive   \n",
       "2  Economical Alternative to Traditional Cleanroo...     Economy  Positive   \n",
       "3  T20 WC: Most economical four-over spells by In...      Sports   Neutral   \n",
       "4  This 2002 Cirrus SR22 G1 Is a Surprisingly Eco...  Economical  Positive   \n",
       "\n",
       "        Aspect  \n",
       "0      Economy  \n",
       "1  Achievement  \n",
       "2    Economics  \n",
       "3      Cricket  \n",
       "4      Economy  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['processed_title'] = df['News Title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding target variables\n",
    "label_encoder_category = LabelEncoder()\n",
    "label_encoder_sentiment = LabelEncoder()\n",
    "\n",
    "df['category_encoded'] = label_encoder_category.fit_transform(df['Category'])\n",
    "df['sentiment_encoded'] = label_encoder_sentiment.fit_transform(df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X = df['processed_title']\n",
    "y_category = df['category_encoded']\n",
    "y_sentiment = df['sentiment_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X, y_category, test_size=0.2, random_state=42)\n",
    "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(X, y_sentiment, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and padding sequences\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat_seq = tokenizer.texts_to_sequences(X_train_cat)\n",
    "X_test_cat_seq = tokenizer.texts_to_sequences(X_test_cat)\n",
    "\n",
    "X_train_sent_seq = tokenizer.texts_to_sequences(X_train_sent)\n",
    "X_test_sent_seq = tokenizer.texts_to_sequences(X_test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "X_train_cat_padded = pad_sequences(X_train_cat_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_cat_padded = pad_sequences(X_test_cat_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "X_train_sent_padded = pad_sequences(X_train_sent_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_sent_padded = pad_sequences(X_test_sent_seq, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 50, 64)            640000    \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 50, 128)           66048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_max_pooling1d_7 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 714629 (2.73 MB)\n",
      "Trainable params: 714629 (2.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.6155 - accuracy: 0.0769 - val_loss: 1.6061 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.5933 - accuracy: 0.1538 - val_loss: 1.6017 - val_accuracy: 0.2500\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5818 - accuracy: 0.4615 - val_loss: 1.5988 - val_accuracy: 0.2500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.5878 - accuracy: 0.4615 - val_loss: 1.5963 - val_accuracy: 0.2500\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5592 - accuracy: 0.5385 - val_loss: 1.5941 - val_accuracy: 0.2500\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5679 - accuracy: 0.5385 - val_loss: 1.5913 - val_accuracy: 0.2500\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.5374 - accuracy: 0.5385 - val_loss: 1.5871 - val_accuracy: 0.2500\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5367 - accuracy: 0.5385 - val_loss: 1.5826 - val_accuracy: 0.2500\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5215 - accuracy: 0.5385 - val_loss: 1.5781 - val_accuracy: 0.2500\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5228 - accuracy: 0.5385 - val_loss: 1.5734 - val_accuracy: 0.2500\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5079 - accuracy: 0.5385 - val_loss: 1.5687 - val_accuracy: 0.2500\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4987 - accuracy: 0.5385 - val_loss: 1.5647 - val_accuracy: 0.2500\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4726 - accuracy: 0.5385 - val_loss: 1.5610 - val_accuracy: 0.2500\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4626 - accuracy: 0.5385 - val_loss: 1.5584 - val_accuracy: 0.2500\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4490 - accuracy: 0.5385 - val_loss: 1.5588 - val_accuracy: 0.2500\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 1.3764 - accuracy: 0.5385 - val_loss: 1.5663 - val_accuracy: 0.2500\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3378 - accuracy: 0.5385 - val_loss: 1.5840 - val_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# Category Prediction Model\n",
    "model_category = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(df['Category'].unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "model_category.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_category.summary()\n",
    "\n",
    "# Train the Category Prediction Model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history_category = model_category.fit(X_train_cat_padded, y_train_cat, epochs=20, batch_size=32,\n",
    "                                     validation_data=(X_test_cat_padded, y_test_cat), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 50, 64)            640000    \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 50, 128)           66048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_max_pooling1d_8 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 714499 (2.73 MB)\n",
      "Trainable params: 714499 (2.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.0979 - accuracy: 0.2308 - val_loss: 1.0970 - val_accuracy: 0.2500\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0844 - accuracy: 0.6154 - val_loss: 1.0986 - val_accuracy: 0.2500\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.0877 - accuracy: 0.5385 - val_loss: 1.0993 - val_accuracy: 0.2500\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0788 - accuracy: 0.6154 - val_loss: 1.1033 - val_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Prediction Model\n",
    "model_sentiment = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(df['Sentiment'].unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "model_sentiment.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_sentiment.summary()\n",
    "\n",
    "# Train the Sentiment Prediction Model\n",
    "history_sentiment = model_sentiment.fit(X_train_sent_padded, y_train_sent, epochs=20, batch_size=32,\n",
    "                                       validation_data=(X_test_sent_padded, y_test_sent), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 831ms/step\n",
      "1/1 [==============================] - 1s 837ms/step\n",
      "Predicted Category: Politics, Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Function to predict category and sentiment for a new title\n",
    "def predict_category_sentiment(title):\n",
    "    processed_title = preprocess_text(title)\n",
    "    seq = tokenizer.texts_to_sequences([processed_title])\n",
    "    padded = pad_sequences(seq, maxlen=max_length, padding='post', truncating='post')\n",
    "    category_encoded = model_category.predict(padded).argmax(axis=-1)[0]\n",
    "    sentiment_encoded = model_sentiment.predict(padded).argmax(axis=-1)[0]\n",
    "    category = label_encoder_category.inverse_transform([category_encoded])[0]\n",
    "    sentiment = label_encoder_sentiment.inverse_transform([sentiment_encoded])[0]\n",
    "    return category, sentiment\n",
    "\n",
    "# Example prediction\n",
    "new_title = \"Lockie Ferguson produces most economical spell in T20 World Cup history\"\n",
    "predicted_category, predicted_sentiment = predict_category_sentiment(new_title)\n",
    "print(f\"Predicted Category: {predicted_category}, Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
